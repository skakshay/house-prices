{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%store -r \n",
    "target = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we saved both the dataframes train and test with dummy variables. Here we retrieve them back. Also remember that the various features are log transformed including SalePrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "Here we try to train gradient boosting model using XGBoost implementation and the sole purpose is to tune the parameters of XGBoost model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain.drop([target], axis=1), label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "#     Fit the algorithm on the data\n",
    "    alg.fit(dtrain.drop([target], axis=1), dtrain[target], eval_metric='rmse')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain.drop([target], axis=1))\n",
    "\n",
    "    #Print model report:\n",
    "    print (\"MSE train : {0}\".format(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    \n",
    "    return cvresult\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firsr we try a high learning rate and choose optimal n_estimators for this learning rate. \n",
    "2. Then we tune tree specific parameters like max_depth, min_child_weight etc for the decided learning_rate and n_estimators. \n",
    "3. Tune regularization parameters\n",
    "4. Lower learning rate and increase n_estimators. \n",
    "\n",
    "### Step 1 : Fix learning_rate and n_estimators for tuning tree base parameters\n",
    "\n",
    "Lets also first set some intial parameters to fix learning_rate and n_estimators\n",
    "1. max_depth=5\n",
    "2. min_child_weight=1\n",
    "3. gamma=0.1\n",
    "4. subsample=0.8\n",
    "5. scale_pos_weight=1\n",
    "6. colsmple_bytree=0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train : 0.005961450553124035\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=1000, min_child_weight=1, subsample=0.8, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.8)\n",
    "cvresult = modelfit(xgb1, train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No estimators : 706\n",
      "RMSE test : 0.12152080000000001\n"
     ]
    }
   ],
   "source": [
    "print (\"No estimators : {0}\".format(xgb1.get_params()['n_estimators']))\n",
    "print (\"RMSE test : {0}\".format(cvresult.iloc[cvresult.shape[0]-1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So n_estimators=706 is the optimal value for learning_rate=0.1\n",
    "Now that we have got our learning_rate and n_estimators fixed lets tune other parameters. \n",
    "\n",
    "#### Tune max_depth and min_child_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 26.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_child_weight': range(1, 6, 2), 'max_depth': range(3, 10, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    "    'max_depth' : range(3, 10, 2),\n",
    "    'min_child_weight' : range(1, 6, 2)\n",
    "}\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, gamma=0.1, colsample_bytree=0.8, subsample=0.8, silent=False\n",
    "                         ,scale_pos_weight=1, max_depth=5, min_child_weight=1)\n",
    "gsearch1 = GridSearchCV(estimator, param_test1, cv=5, verbose=True)\n",
    "gsearch1.fit(train_dummy.drop([target], axis=1), train_dummy[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So max_depth = 5 and min_child_weight=3 are the best values for these parameters. Lets get more precise values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_child_weight': [2, 3, 4], 'max_depth': [4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    "    'max_depth' : [4,5,6],\n",
    "    'min_child_weight' : [2,3,4]\n",
    "}\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, gamma=0.1, colsample_bytree=0.8, subsample=0.8, silent=False\n",
    "                         ,scale_pos_weight=1)\n",
    "gsearch2 = GridSearchCV(estimator, param_test2, cv=5, verbose=True)\n",
    "gsearch2.fit(train_dummy.drop([target], axis=1), train_dummy[target])\n",
    "gsearch2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the optimal values for max_depth=5 and min_child_weight=3. \n",
    "#### Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 15.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3={'gamma':[i/10 for i in range(0,5)]}\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, colsample_bytree=0.8, subsample=0.8, silent=False\n",
    "                         ,scale_pos_weight=1, max_depth=5, min_child_weight=3)\n",
    "gsearch3 = GridSearchCV(estimator, param_test3, cv=5, verbose=True)\n",
    "gsearch3.fit(train_dummy.drop([target], axis=1), train_dummy[target])\n",
    "\n",
    "gsearch3.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So optimal gamma=0.1\n",
    "\n",
    "####  Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.7)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4={'subsample':[i/10 for i in range(6,10)]\n",
    "             , 'colsample_bytree':[i/10 for i in range(6,10)]}\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, silent=False, gamma=0.1\n",
    "                         ,scale_pos_weight=1, max_depth=5, min_child_weight=3)\n",
    "gsearch4 = GridSearchCV(estimator, param_test4, cv=5, verbose=True)\n",
    "gsearch4.fit(train_dummy.drop([target], axis=1), train_dummy[target])\n",
    "\n",
    "gsearch4.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsmaple=0.7, colsample_bytree=0.8\n",
    "\n",
    "Now we can try value in the range of 0.05 around the found optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'subsample': [0.65, 0.68, 0.7, 0.72, 0.75], 'colsample_bytree': [0.75, 0.78, 0.8, 0.82, 0.85]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.72)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {'subsample':[0.65, 0.68, 0.7, 0.72, 0.75]\n",
    "             , 'colsample_bytree':[0.75, 0.78, 0.8, 0.82, 0.85]}\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, silent=False, gamma=0.1\n",
    "                         ,scale_pos_weight=1, max_depth=5, min_child_weight=3)\n",
    "gsearch5 = GridSearchCV(estimator, param_test5, cv=5, verbose=True)\n",
    "gsearch5.fit(train_dummy.drop([target], axis=1), train_dummy[target])\n",
    "\n",
    "gsearch5.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsample=0.72, colsample_bytree=0.75\n",
    "\n",
    "#### Tuning regularization parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.72),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=706, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=1e-05, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.72)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test6 = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100] }\n",
    "estimator = XGBRegressor(learning_rate=0.1, n_estimators=706, silent=False, gamma=0.1\n",
    "                         ,scale_pos_weight=1, max_depth=5, min_child_weight=3, subsample=0.72, colsample_bytree=0.75)\n",
    "gsearch6 = GridSearchCV(estimator, param_test6, cv=5, verbose=True)\n",
    "gsearch6.fit(train_dummy.drop([target], axis=1), train_dummy[target])\n",
    "\n",
    "gsearch6.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reg_alpha = 1e-5\n",
    "This may be because most of complexity is controlled by max_depth and gamma. \n",
    "Now lets first find the optimal n_estimators for updated parameters, then we will reduce learning rate and increase n_estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No estimators : 885\n",
      "RMSE test : 0.12089839999999999\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=1000, min_child_weight=3, subsample=0.72, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.75, reg_alpha=0.00001)\n",
    "cvresult2 = modelfit(xgb2, train_dummy)\n",
    "print (\"No estimators : {0}\".format(xgb2.get_params()['n_estimators']))\n",
    "print (\"RMSE test : {0}\".format(cvresult2.iloc[cvresult2.shape[0]-1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets reduce learning rate and increase n_estimators proportionally.\n",
    "learning_rate=0.05  n_estimators=1670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train : 0.006505656325214961\n"
     ]
    }
   ],
   "source": [
    "xgb3 = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=8850, min_child_weight=3, subsample=0.72, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.75, reg_alpha=0.00001)\n",
    "cvresult3 = modelfit(xgb3, train_dummy)\n",
    "print (\"No estimators : {0}\".format(xgb3.get_params()['n_estimators']))\n",
    "print (\"RMSE test : {0}\".format(cvresult3.iloc[cvresult3.shape[0]-1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No estimators : 3602\n",
      "RMSE test : 0.1184584\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets half our learning_rate and double optimal n_estimators.\n",
    "1. learning_rate=0.005\n",
    "2. n_estimators=7204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train : 0.006641313921785242\n",
      "No estimators : 5255\n",
      "RMSE test : 0.119139\n"
     ]
    }
   ],
   "source": [
    "xgb4 = XGBRegressor(max_depth=5, learning_rate=0.005, n_estimators=7204, min_child_weight=3, subsample=0.72, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.75, reg_alpha=0.00001)\n",
    "cvresult4 = modelfit(xgb4, train_dummy)\n",
    "print (\"No estimators : {0}\".format(xgb4.get_params()['n_estimators']))\n",
    "print (\"RMSE test : {0}\".format(cvresult4.iloc[cvresult4.shape[0]-1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here our test error is more than pervious one where learning_rate was 0.01, let try a mid value and see if we can reduce it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train : 0.006536573940670113\n",
      "No estimators : 3672\n",
      "RMSE test : 0.11846940000000002\n"
     ]
    }
   ],
   "source": [
    "xgb5 = XGBRegressor(max_depth=5, learning_rate=0.009, n_estimators=4000, min_child_weight=3, subsample=0.72, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.75, reg_alpha=0.00001)\n",
    "cvresult5 = modelfit(xgb5, train_dummy)\n",
    "print (\"No estimators : {0}\".format(xgb5.get_params()['n_estimators']))\n",
    "print (\"RMSE test : {0}\".format(cvresult5.iloc[cvresult5.shape[0]-1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even RMSE test with learning_rate=0.009 is greater than learning_rate=0.01\n",
    "So our optimal parameters are n_estimators=3602, learning_rate=0.01. Lets fit and predict with final parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0.1, learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=3, missing=None, n_estimators=3602, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=1e-05, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=0.72)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_final = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=3602, min_child_weight=3, subsample=0.72, scale_pos_weight=1\n",
    "                   , silent=False, gamma=0.1, colsample_bytree=0.75, reg_alpha=0.00001)\n",
    "xgb_final.fit(train_dummy.drop([target], axis=1), train_dummy[target], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_final.predict(test_dummy)\n",
    "y_pred = np.expm1(y_pred)\n",
    "df_xbg = pd.DataFrame(y_pred, index=test.index, columns=(['SalePrice']))\n",
    "df_xbg.to_csv('./submissions/xgb.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
